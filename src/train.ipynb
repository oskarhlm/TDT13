{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT13 Project - Oskar Holm (F2023)\n",
    "\n",
    "This project is based on the shared task related to Social Media Geolocation (SMG) from VarDial 2020 and 2021, specifically the Workshop on Natural Language Processing (NLP) for Similar Languages, Varieties, and Dialects. Unlike typical VarDial tasks that involve choosing from a set of variety labels, this task focuses on predicting the latitude and longitude from which a social media post was made.\n",
    "\n",
    "The task remained the same in both 2020 and 2021, covering three language areas: Bosnian-Croatian-Montenegrin-Serbian, German (Germany and Austria), and German-speaking Switzerland. This project is limited to the German-speaking Switzerland area due to time constraints and resource availability.\n",
    "\n",
    "The goal of the project is to replicate the results of a study that used a BERT-based classifier for this double regression task. The dataset from the 2020 VarDial challenge is chosen because it had more submissions compared to the 2021 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import json \n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "import random\n",
    "\n",
    "from lib.preprocessing import scalers, get_reduced_dev_split\n",
    "from lib.train_utils import TensorBoardCheckpoint, get_model, get_lossfn, get_scheduler, evaluate_geolocation_model_by_checkpoint\n",
    "from lib.geo import to_projection, GeolocationDataset\n",
    "from lib.metrics import median_distance, mean_distance\n",
    "from lib.plotting import plot_switzerland, plot_barchart\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "data_path = '../data'\n",
    "vardial_path = '../vardial-shared-tasks/SMG2020'\n",
    "\n",
    "config_name = 'bert-finetuned-swiss-L1-reduced-dev-plateau'\n",
    "with open('./configs.json', 'r') as f: \n",
    "    configs = json.load(f)\n",
    "\n",
    "config = configs[config_name]\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_table(f'{vardial_path}/ch/train.txt', header=None, names=['lat', 'lon', 'text'])\n",
    "dev_data = pd.read_table(f'{vardial_path}/ch/dev.txt', header=None, names=['lat', 'lon', 'text'])\n",
    "test_gold_data = pd.read_table(f'{vardial_path}/ch/test_gold.txt', header=None, names=['lat', 'lon', 'text'])\n",
    "\n",
    "# Get alternative split, if specified in config\n",
    "if 'split' in config and config['split'] == 'reduced-dev':\n",
    "    train_data, dev_data = get_reduced_dev_split(train_data, dev_data)\n",
    "\n",
    "# Convert to specified projection, if any\n",
    "train_data, col_names = to_projection(train_data, config)\n",
    "dev_data, _ = to_projection(dev_data, config)\n",
    "test_gold_data, _ = to_projection(test_gold_data, config)\n",
    "\n",
    "# Scaling\n",
    "scaler = scalers[config['scaler']]()\n",
    "train_coords = scaler.fit_transform(train_data[col_names[:2]].values)\n",
    "Path(f'{data_path}/ch').mkdir(exist_ok=True)\n",
    "with open(f'{data_path}/ch/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Scale other datasets and create loaders\n",
    "train_dataset = GeolocationDataset(train_data['text'].tolist(), train_coords, config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True)\n",
    "\n",
    "dev_coords = scaler.transform(dev_data[col_names[:2]].values)\n",
    "dev_dataset = GeolocationDataset(dev_data['text'].tolist(), dev_coords, config)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "test_gold_coords = scaler.transform(test_gold_data[col_names[:2]].values)\n",
    "test_gold_dataset = GeolocationDataset(test_gold_data['text'].tolist(), test_gold_coords, config)\n",
    "test_gold_loader = DataLoader(test_gold_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = get_model(config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), config['lr'])\n",
    "if 'scheduler' in config:\n",
    "    scheduler = get_scheduler(optimizer, config)\n",
    "    \n",
    "loss_function = get_lossfn(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_checkpoint = TensorBoardCheckpoint(log_dir=f'{data_path}/ch/logs',\n",
    "                                      checkpoint_path=f'{data_path}/ch/checkpoints', run_name=config_name)\n",
    "\n",
    "def train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=10):\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "    # Early stopping\n",
    "    best_metric = float('inf')  \n",
    "    epochs_no_improve = 0\n",
    "    early_stop_patience = 10  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on dev (validation)\n",
    "        model.eval()\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader):\n",
    "                inputs, labels = batch\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(**inputs)\n",
    "                dev_preds.append(outputs.logits.cpu().numpy())\n",
    "                dev_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        dev_preds = np.vstack(dev_preds)\n",
    "        dev_labels = np.vstack(dev_labels)\n",
    "\n",
    "        median_dist = median_distance(dev_preds, dev_labels, scaler, config)\n",
    "        mean_dist = mean_distance(dev_preds, dev_labels, scaler, config)\n",
    "\n",
    "        metrics = {\n",
    "            'Loss/train': avg_train_loss, \n",
    "            'Median_Distance/dev': median_dist, \n",
    "            'Mean_Distance/dev': mean_dist,\n",
    "        }\n",
    "        tb_checkpoint.log_metrics(metrics, epoch)\n",
    "        tb_checkpoint.save_checkpoint(model, optimizer, epoch, metrics, scaler)\n",
    "\n",
    "        current_metric = metrics['Median_Distance/dev'] \n",
    "        if current_metric < best_metric:\n",
    "            best_metric = current_metric\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stop_patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        if 'scheduler' in config:\n",
    "            scheduler.step(metrics['Median_Distance/dev'])\n",
    "\n",
    "with open(f'{data_path}/ch/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=config['epochs'])\n",
    "\n",
    "tb_checkpoint.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Evaluate and compare all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkp_dir = f'{data_path}/ch/checkpoints'\n",
    "\n",
    "config_names = configs.keys()\n",
    "\n",
    "best_results = {\n",
    "    'median_distance': np.inf,\n",
    "    'mean_distance': np.inf\n",
    "}\n",
    "\n",
    "for config_name in config_names:\n",
    "    chkp_file = f'{config_name}_best_model.pth'\n",
    "    chkp_config = configs[config_name]\n",
    "\n",
    "    results, _ = evaluate_geolocation_model_by_checkpoint(\n",
    "        chkp_dir,\n",
    "        chkp_file,\n",
    "        vardial_path,\n",
    "        chkp_config,\n",
    "    )\n",
    "\n",
    "    if results['median_distance'] < best_results['median_distance']:\n",
    "        best_checkpoint = chkp_file\n",
    "        best_results['median_distance'] = results['median_distance']\n",
    "        best_results['mean_distance'] = results['mean_distance']\n",
    "\n",
    "print(f\"{'-' * 40}\\nBest Checkpoint:\", best_checkpoint)\n",
    "print(\"Best Results:\", best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_name = '20231107-230346'\n",
    "# config_name = 'utm_lr2e-5'\n",
    "# config_name = 'swissbert'\n",
    "\n",
    "chkp_file = f'{config_name}_best_model.pth'\n",
    "chkp_config = configs[config_name]\n",
    "\n",
    "_, test_preds = evaluate_geolocation_model_by_checkpoint(\n",
    "    chkp_dir,\n",
    "    chkp_file,\n",
    "    vardial_path,\n",
    "    chkp_config,\n",
    ")\n",
    "\n",
    "test_gold_data = pd.read_table(f'{vardial_path}/ch/test_gold.txt', header=None, names=['lat', 'lon', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_switzerland(test_preds, test_gold_data, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barchart(test_preds, test_gold_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
