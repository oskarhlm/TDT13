@inproceedings{benitesdeazevedoesouzaZHAWInITSocialMedia2020,
  title = {{{ZHAW-InIT}} : Social Media Geolocation at {{VarDial}} 2020},
  shorttitle = {{{ZHAW-InIT}}},
  booktitle = {Workshop on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}, {{Barcelona}} ({{Spain}}), Online, 13 {{December}} 2020},
  author = {{Benites de Azevedo e Souza}, Fernando and H{\"u}rlimann, Manuela and {von D{\"a}niken}, Pius and Cieliebak, Mark},
  year = {2020},
  month = dec,
  pages = {254--264},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  doi = {10.21256/zhaw-21551},
  urldate = {2023-10-06},
  abstract = {We describe our approaches for the Social Media Geolocation (SMG) task at the VarDial Evaluation Campaign 2020. The goal was to predict geographical location (latitudes and longitudes) given an input text. There were three subtasks corresponding to German-speaking Switzerland (CH), Germany and Austria (DE-AT), and Croatia, Bosnia and Herzegovina, Montenegro and Serbia (BCMS). We submitted solutions to all subtasks but focused our development efforts on the CH subtask, where we achieved third place out of 16 submissions with a median distance of 15.93 km and had the best result of 14 unconstrained systems. In the DE-AT subtask, we ranked sixth out of ten submissions (fourth of 8 unconstrained systems) and for BCMS we achieved fourth place out of 13 submissions (second of 11 unconstrained systems).},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  isbn = {978-1-952148-47-7},
  langid = {english},
  annotation = {Accepted: 2021-02-04T13:13:06Z},
  file = {C:\Users\oskar\Zotero\storage\KMIQXNQG\Benites de Azevedo e Souza et al. - 2020 - ZHAW-InIT  social media geolocation at VarDial 20.pdf}
}

@inproceedings{chakravarthiFindingsVarDialEvaluation2021,
  title = {Findings of the {{VarDial Evaluation Campaign}} 2021},
  booktitle = {Proceedings of the {{Eighth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Chakravarthi, Bharathi Raja and Mihaela, Gaman and Ionescu, Radu Tudor and Jauhiainen, Heidi and Jauhiainen, Tommi and Lind{\'e}n, Krister and Ljube{\v s}i{\'c}, Nikola and Partanen, Niko and Priyadharshini, Ruba and Purschke, Christoph and Rajagopal, Eswari and Scherrer, Yves and Zampieri, Marcos},
  year = {2021},
  month = apr,
  pages = {1--11},
  publisher = {{Association for Computational Linguistics}},
  address = {{Kiyv, Ukraine}},
  urldate = {2023-10-16},
  abstract = {This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns.},
  file = {C:\Users\oskar\Zotero\storage\GLQTG498\Chakravarthi et al. - 2021 - Findings of the VarDial Evaluation Campaign 2021.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-10-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\G4I8TCLX\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\oskar\\Zotero\\storage\\AFC5RD5Z\\1810.html}
}

@misc{federalofficeoftopographyswisstopoSwissCoordinatesSystem,
  title = {The {{Swiss}} Coordinates System},
  author = {{Federal Office of Topography swisstopo}},
  journal = {Federal Office of Topography swisstopo},
  urldate = {2023-11-17},
  abstract = {swisstopo specified the control points in the Swiss coordinates system within the scope of the 1995 national survey (LV95) and with the aid of satellite technology and the global positioning system (GPS).},
  howpublished = {https://www.swisstopo.admin.ch/en/knowledge-facts/surveying-geodesy/coordinates/swiss-coordinates.html},
  langid = {english},
  file = {C:\Users\oskar\Zotero\storage\IHVGL4VC\swiss-coordinates.html}
}

@inproceedings{gamanReportVarDialEvaluation2020,
  title = {A {{Report}} on the {{VarDial Evaluation Campaign}} 2020},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Gaman, Mihaela and Hovy, Dirk and Ionescu, Radu Tudor and Jauhiainen, Heidi and Jauhiainen, Tommi and Lind{\'e}n, Krister and Ljube{\v s}i{\'c}, Nikola and Partanen, Niko and Purschke, Christoph and Scherrer, Yves and Zampieri, Marcos},
  year = {2020},
  month = dec,
  pages = {1--14},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  address = {{Barcelona, Spain (Online)}},
  urldate = {2023-10-16},
  abstract = {This paper presents the results of the VarDial Evaluation Campaign 2020 organized as part of the seventh workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with COLING 2020. The campaign included three shared tasks each focusing on a different challenge of language and dialect identification: Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). The campaign attracted 30 teams who enrolled to participate in one or multiple shared tasks and 14 of them submitted runs across the three shared tasks. Finally, 11 papers describing participating systems are published in the VarDial proceedings and referred to in this report.},
  file = {C:\Users\oskar\Zotero\storage\8UCGMNYC\Gaman et al. - 2020 - A Report on the VarDial Evaluation Campaign 2020.pdf}
}

@misc{guillaumeTextbasedGeolocationPrediction2023,
  title = {Text-Based {{Geolocation Prediction}} of {{Twitter Users}}},
  author = {Guillaume, Giovanni},
  year = {2023},
  month = aug,
  urldate = {2023-10-06},
  abstract = {This project takes on the goal to improve upon Yachay.ai's infrastructure to train a deep learning model to predict coordinates of individual texts.}
}

@inproceedings{hovyCapturingRegionalVariation2018,
  title = {Capturing {{Regional Variation}} with {{Distributed Place Representations}} and {{Geographic Retrofitting}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hovy, Dirk and Purschke, Christoph},
  editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
  year = {2018},
  month = oct,
  pages = {4383--4394},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1469},
  urldate = {2023-11-17},
  abstract = {Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.},
  file = {C:\Users\oskar\Zotero\storage\4IUXHBDS\Hovy and Purschke - 2018 - Capturing Regional Variation with Distributed Plac.pdf}
}

@article{huldenKernelDensityEstimation2015,
  title = {Kernel {{Density Estimation}} for {{Text-Based Geolocation}}},
  author = {Hulden, Mans and Silfverberg, Miikka and Francom, Jerid},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v29i1.9149},
  urldate = {2023-10-16},
  abstract = {Text-based geolocation classifiers often operate with a grid-based view of the world.  Predicting document location of origin based on text content on a geodesic grid is computationally attractive since many standard methods for supervised document classification carry over unchanged to geolocation in the form of predicting a most probable grid cell for a document.  However, the grid-based approach suffers from sparse data problems if one wants to improve classification accuracy by moving to smaller cell sizes.  In this paper we investigate an enhancement of common methods for determining the geographic point of origin of a text document by kernel density estimation.  For geolocation of tweets we obtain a  improvements upon non-kernel methods on datasets of U.S. and global Twitter content.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {kernel classifier},
  file = {C:\Users\oskar\Zotero\storage\TTUSZ3H6\Hulden et al. - 2015 - Kernel Density Estimation for Text-Based Geolocati.pdf}
}

@misc{linderAutomaticCreationText2020,
  title = {Automatic {{Creation}} of {{Text Corpora}} for {{Low-Resource Languages}} from the {{Internet}}: {{The Case}} of {{Swiss German}}},
  shorttitle = {Automatic {{Creation}} of {{Text Corpora}} for {{Low-Resource Languages}} from the {{Internet}}},
  author = {Linder, Lucy and Jungo, Michael and Hennebert, Jean and Musat, Claudiu and Fischer, Andreas},
  year = {2020},
  month = jun,
  number = {arXiv:1912.00159},
  eprint = {1912.00159},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.00159},
  urldate = {2023-11-17},
  abstract = {This paper presents SwissCrawl, the largest Swiss German text corpus to date. Composed of more than half a million sentences, it was generated using a customized web scraping tool that could be applied to other low-resource languages as well. The approach demonstrates how freely available web pages can be used to construct comprehensive text corpora, which are of fundamental importance for natural language processing. In an experimental evaluation, we show that using the new corpus leads to significant improvements for the task of language modeling. To capture new content, our approach will run continuously to keep increasing the corpus over time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\89I5DRES\\Linder et al. - 2020 - Automatic Creation of Text Corpora for Low-Resourc.pdf;C\:\\Users\\oskar\\Zotero\\storage\\FRA65EJ5\\1912.html}
}

@inproceedings{ljubesicTweetGeoToolCollecting2016,
  title = {{{TweetGeo}} - {{A Tool}} for {{Collecting}}, {{Processing}} and {{Analysing Geo-encoded Linguistic Data}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Ljube{\v s}i{\'c}, Nikola and Samard{\v z}i{\'c}, Tanja and Derungs, Curdin},
  year = {2016},
  month = dec,
  pages = {3412--3421},
  publisher = {{The COLING 2016 Organizing Committee}},
  address = {{Osaka, Japan}},
  urldate = {2023-10-13},
  abstract = {In this paper we present a newly developed tool that enables researchers interested in spatial variation of language to define a geographic perimeter of interest, collect data from the Twitter streaming API published in that perimeter, filter the obtained data by language and country, define and extract variables of interest and analyse the extracted variables by one spatial statistic and two spatial visualisations. We showcase the tool on the area and a selection of languages spoken in former Yugoslavia. By defining the perimeter, languages and a series of linguistic variables of interest we demonstrate the data collection, processing and analysis capabilities of the tool.},
  file = {C:\Users\oskar\Zotero\storage\DJJBDJ6G\Ljubešić et al. - 2016 - TweetGeo - A Tool for Collecting, Processing and A.pdf}
}

@article{lloydLeastSquaresQuantization1982a,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2023-11-19},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\^bquanta,b=1,2, \textbackslash cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\76GNK79P\\Lloyd - 1982 - Least squares quantization in PCM.pdf;C\:\\Users\\oskar\\Zotero\\storage\\M72ZPEWY\\1056489.html}
}

@article{maehlumUTM2023,
  title = {{UTM}},
  author = {M{\ae}hlum, Lars},
  year = {2023},
  month = jun,
  journal = {Store norske leksikon},
  urldate = {2023-11-17},
  abstract = {UTM er en kartprojeksjon og et koordinatsystem i utstrakt internasjonal bruk, ogs\aa{} i Norge. Projeksjonen bygger p\aa{} Gauss-Kr\"ugers projeksjon, som er en transversal sylinderprojeksjon der projeksjonsplanet tangerer jordkloden langs en meridian, ikke langs ekvator. .},
  copyright = {begrenset},
  langid = {norsk},
  keywords = {Datum og kartprojeksjon},
  file = {C:\Users\oskar\Zotero\storage\WX83LAUW\UTM.html}
}

@inproceedings{pfeifferLiftingCurseMultilinguality2022,
  title = {Lifting the {{Curse}} of {{Multilinguality}} by {{Pre-training Modular Transformers}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Pfeiffer, Jonas and Goyal, Naman and Lin, Xi and Li, Xian and Cross, James and Riedel, Sebastian and Artetxe, Mikel},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  pages = {3479--3495},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.255},
  urldate = {2023-11-16},
  abstract = {Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.},
  file = {C:\Users\oskar\Zotero\storage\E9JIW4LV\Pfeiffer et al. - 2022 - Lifting the Curse of Multilinguality by Pre-traini.pdf}
}

@misc{philippeGeolocation2022,
  title = {Geolocation},
  author = {Philippe},
  year = {2022},
  month = jul,
  urldate = {2023-10-06},
  abstract = {Geolocation prediction for a given Tweet},
  copyright = {GPL-3.0},
  keywords = {geolocation,geolocation-prediction,neural-network,pretrained-models,tweets}
}

@article{roynelandDialectsNorwayCatching2009,
  title = {Dialects in {{Norway}}: Catching up with the Rest of {{Europe}}?},
  shorttitle = {Dialects in {{Norway}}},
  author = {R{\o}yneland, Unn},
  year = {2009},
  month = mar,
  volume = {2009},
  number = {196-197},
  pages = {7--30},
  publisher = {{De Gruyter Mouton}},
  issn = {1613-3668},
  doi = {10.1515/IJSL.2009.015},
  urldate = {2023-11-10},
  abstract = {Norway has sometimes been described as a sociolinguistic paradise with its abundant linguistic heterogeneity \textemdash{} both written and spoken. Dialect diversity has been and is still considerable and dialects are used in practically all social domains. However, dialects in Norway are changing. In this article I will discuss the historical background for the linguistic situation in Norway, and I will take a closer look at present-day developments and discuss the structural, sociocultural, and psychological mechanisms behind them. The question is whether the dialect situation in Norway remains very different from most other parts of Europe, or if at least some areas of Norway may be experiencing similar developments.},
  chapter = {International Journal of the Sociology of Language},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {C:\Users\oskar\Zotero\storage\2UZVYRLX\Røyneland - 2009 - Dialects in Norway catching up with the rest of E.pdf}
}

@inproceedings{samuelNorBenchBenchmarkNorwegian2023,
  title = {{{NorBench}} \textendash{} {{A Benchmark}} for {{Norwegian Language Models}}},
  booktitle = {Proceedings of the 24th {{Nordic Conference}} on {{Computational Linguistics}} ({{NoDaLiDa}})},
  author = {Samuel, David and Kutuzov, Andrey and Touileb, Samia and Velldal, Erik and {\O}vrelid, Lilja and R{\o}nningstad, Egil and Sigdel, Elina and Palatkina, Anna},
  year = {2023},
  month = may,
  pages = {618--633},
  publisher = {{University of Tartu Library}},
  address = {{T\'orshavn, Faroe Islands}},
  urldate = {2023-10-16},
  abstract = {We present NorBench: a streamlined suite of NLP tasks and probes for evaluating Norwegian language models (LMs) on standardized data splits and evaluation metrics. We also introduce a range of new Norwegian language models (both encoder and encoder-decoder based). Finally, we compare and analyze their performance, along with other existing LMs, across the different benchmark tests of NorBench.},
  file = {C:\Users\oskar\Zotero\storage\XZJREB9E\Samuel et al. - 2023 - NorBench – A Benchmark for Norwegian Language Mode.pdf}
}

@inproceedings{scherrerHeLjuVarDial20202020,
  title = {{{HeLju}}@{{VarDial}} 2020: {{Social Media Variety Geolocation}} with {{BERT Models}}},
  shorttitle = {{{HeLju}}@{{VarDial}} 2020},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Scherrer, Yves and Ljube{\v s}i{\'c}, Nikola},
  year = {2020},
  month = dec,
  pages = {202--211},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  address = {{Barcelona, Spain (Online)}},
  urldate = {2023-10-16},
  abstract = {This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our models reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional models by far, and that improvements obtained by pre-training models on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction: named entities and dialectal features, both of which are handled well by our models.},
  file = {C:\Users\oskar\Zotero\storage\SXY6K8F7\Scherrer and Ljubešić - 2020 - HeLju@VarDial 2020 Social Media Variety Geolocati.pdf}
}

@inproceedings{scherrerSocialMediaVariety2021,
  title = {Social {{Media Variety Geolocation}} with {{geoBERT}}},
  booktitle = {Proceedings of the {{Eighth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Scherrer, Yves and Ljube{\v s}i{\'c}, Nikola},
  year = {2021},
  month = apr,
  pages = {135--140},
  publisher = {{Association for Computational Linguistics}},
  address = {{Kiyv, Ukraine}},
  urldate = {2023-10-06},
  abstract = {This paper describes the Helsinki\textendash Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available.},
  file = {C:\Users\oskar\Zotero\storage\5RVDU6NE\Scherrer and Ljubešić - 2021 - Social Media Variety Geolocation with geoBERT.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-10},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\oskar\Zotero\storage\7YZDRE5Z\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}
