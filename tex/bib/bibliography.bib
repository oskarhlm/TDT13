@inproceedings{benitesdeazevedoesouzaZHAWInITSocialMedia2020,
  title = {{{ZHAW-InIT}} : Social Media Geolocation at {{VarDial}} 2020},
  shorttitle = {{{ZHAW-InIT}}},
  booktitle = {Workshop on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}, {{Barcelona}} ({{Spain}}), Online, 13 {{December}} 2020},
  author = {{Benites de Azevedo e Souza}, Fernando and H{\"u}rlimann, Manuela and {von D{\"a}niken}, Pius and Cieliebak, Mark},
  year = {2020},
  month = dec,
  pages = {254--264},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  doi = {10.21256/zhaw-21551},
  urldate = {2023-10-06},
  abstract = {We describe our approaches for the Social Media Geolocation (SMG) task at the VarDial Evaluation Campaign 2020. The goal was to predict geographical location (latitudes and longitudes) given an input text. There were three subtasks corresponding to German-speaking Switzerland (CH), Germany and Austria (DE-AT), and Croatia, Bosnia and Herzegovina, Montenegro and Serbia (BCMS). We submitted solutions to all subtasks but focused our development efforts on the CH subtask, where we achieved third place out of 16 submissions with a median distance of 15.93 km and had the best result of 14 unconstrained systems. In the DE-AT subtask, we ranked sixth out of ten submissions (fourth of 8 unconstrained systems) and for BCMS we achieved fourth place out of 13 submissions (second of 11 unconstrained systems).},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  isbn = {978-1-952148-47-7},
  langid = {english},
  annotation = {Accepted: 2021-02-04T13:13:06Z},
  file = {C:\Users\oskar\Zotero\storage\KMIQXNQG\Benites de Azevedo e Souza et al. - 2020 - ZHAW-InIT  social media geolocation at VarDial 20.pdf}
}

@inproceedings{chakravarthiFindingsVarDialEvaluation2021,
  title = {Findings of the {{VarDial Evaluation Campaign}} 2021},
  booktitle = {Proceedings of the {{Eighth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Chakravarthi, Bharathi Raja and Mihaela, Gaman and Ionescu, Radu Tudor and Jauhiainen, Heidi and Jauhiainen, Tommi and Lind{\'e}n, Krister and Ljube{\v s}i{\'c}, Nikola and Partanen, Niko and Priyadharshini, Ruba and Purschke, Christoph and Rajagopal, Eswari and Scherrer, Yves and Zampieri, Marcos},
  year = {2021},
  month = apr,
  pages = {1--11},
  publisher = {{Association for Computational Linguistics}},
  address = {{Kiyv, Ukraine}},
  urldate = {2023-10-16},
  abstract = {This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns.},
  file = {C:\Users\oskar\Zotero\storage\GLQTG498\Chakravarthi et al. - 2021 - Findings of the VarDial Evaluation Campaign 2021.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-10-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\oskar\\Zotero\\storage\\G4I8TCLX\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\oskar\\Zotero\\storage\\AFC5RD5Z\\1810.html}
}

@inproceedings{gamanReportVarDialEvaluation2020,
  title = {A {{Report}} on the {{VarDial Evaluation Campaign}} 2020},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Gaman, Mihaela and Hovy, Dirk and Ionescu, Radu Tudor and Jauhiainen, Heidi and Jauhiainen, Tommi and Lind{\'e}n, Krister and Ljube{\v s}i{\'c}, Nikola and Partanen, Niko and Purschke, Christoph and Scherrer, Yves and Zampieri, Marcos},
  year = {2020},
  month = dec,
  pages = {1--14},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  address = {{Barcelona, Spain (Online)}},
  urldate = {2023-10-16},
  abstract = {This paper presents the results of the VarDial Evaluation Campaign 2020 organized as part of the seventh workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with COLING 2020. The campaign included three shared tasks each focusing on a different challenge of language and dialect identification: Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). The campaign attracted 30 teams who enrolled to participate in one or multiple shared tasks and 14 of them submitted runs across the three shared tasks. Finally, 11 papers describing participating systems are published in the VarDial proceedings and referred to in this report.},
  file = {C:\Users\oskar\Zotero\storage\8UCGMNYC\Gaman et al. - 2020 - A Report on the VarDial Evaluation Campaign 2020.pdf}
}

@misc{guillaumeTextbasedGeolocationPrediction2023,
  title = {Text-Based {{Geolocation Prediction}} of {{Twitter Users}}},
  author = {Guillaume, Giovanni},
  year = {2023},
  month = aug,
  urldate = {2023-10-06},
  abstract = {This project takes on the goal to improve upon Yachay.ai's infrastructure to train a deep learning model to predict coordinates of individual texts.}
}

@article{huldenKernelDensityEstimation2015,
  title = {Kernel {{Density Estimation}} for {{Text-Based Geolocation}}},
  author = {Hulden, Mans and Silfverberg, Miikka and Francom, Jerid},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v29i1.9149},
  urldate = {2023-10-16},
  abstract = {Text-based geolocation classifiers often operate with a grid-based view of the world.  Predicting document location of origin based on text content on a geodesic grid is computationally attractive since many standard methods for supervised document classification carry over unchanged to geolocation in the form of predicting a most probable grid cell for a document.  However, the grid-based approach suffers from sparse data problems if one wants to improve classification accuracy by moving to smaller cell sizes.  In this paper we investigate an enhancement of common methods for determining the geographic point of origin of a text document by kernel density estimation.  For geolocation of tweets we obtain a  improvements upon non-kernel methods on datasets of U.S. and global Twitter content.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {kernel classifier},
  file = {C:\Users\oskar\Zotero\storage\TTUSZ3H6\Hulden et al. - 2015 - Kernel Density Estimation for Text-Based Geolocati.pdf}
}

@inproceedings{ljubesicTweetGeoToolCollecting2016,
  title = {{{TweetGeo}} - {{A Tool}} for {{Collecting}}, {{Processing}} and {{Analysing Geo-encoded Linguistic Data}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Ljube{\v s}i{\'c}, Nikola and Samard{\v z}i{\'c}, Tanja and Derungs, Curdin},
  year = {2016},
  month = dec,
  pages = {3412--3421},
  publisher = {{The COLING 2016 Organizing Committee}},
  address = {{Osaka, Japan}},
  urldate = {2023-10-13},
  abstract = {In this paper we present a newly developed tool that enables researchers interested in spatial variation of language to define a geographic perimeter of interest, collect data from the Twitter streaming API published in that perimeter, filter the obtained data by language and country, define and extract variables of interest and analyse the extracted variables by one spatial statistic and two spatial visualisations. We showcase the tool on the area and a selection of languages spoken in former Yugoslavia. By defining the perimeter, languages and a series of linguistic variables of interest we demonstrate the data collection, processing and analysis capabilities of the tool.},
  file = {C:\Users\oskar\Zotero\storage\DJJBDJ6G\Ljubešić et al. - 2016 - TweetGeo - A Tool for Collecting, Processing and A.pdf}
}

@misc{philippeGeolocation2022,
  title = {Geolocation},
  author = {Philippe},
  year = {2022},
  month = jul,
  urldate = {2023-10-06},
  abstract = {Geolocation prediction for a given Tweet},
  copyright = {GPL-3.0},
  keywords = {geolocation,geolocation-prediction,neural-network,pretrained-models,tweets}
}

@article{roynelandDialectsNorwayCatching2009,
  title = {Dialects in {{Norway}}: Catching up with the Rest of {{Europe}}?},
  shorttitle = {Dialects in {{Norway}}},
  author = {R{\o}yneland, Unn},
  year = {2009},
  month = mar,
  volume = {2009},
  number = {196-197},
  pages = {7--30},
  publisher = {{De Gruyter Mouton}},
  issn = {1613-3668},
  doi = {10.1515/IJSL.2009.015},
  urldate = {2023-11-10},
  abstract = {Norway has sometimes been described as a sociolinguistic paradise with its abundant linguistic heterogeneity \textemdash{} both written and spoken. Dialect diversity has been and is still considerable and dialects are used in practically all social domains. However, dialects in Norway are changing. In this article I will discuss the historical background for the linguistic situation in Norway, and I will take a closer look at present-day developments and discuss the structural, sociocultural, and psychological mechanisms behind them. The question is whether the dialect situation in Norway remains very different from most other parts of Europe, or if at least some areas of Norway may be experiencing similar developments.},
  chapter = {International Journal of the Sociology of Language},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {C:\Users\oskar\Zotero\storage\2UZVYRLX\Røyneland - 2009 - Dialects in Norway catching up with the rest of E.pdf}
}

@inproceedings{samuelNorBenchBenchmarkNorwegian2023,
  title = {{{NorBench}} \textendash{} {{A Benchmark}} for {{Norwegian Language Models}}},
  booktitle = {Proceedings of the 24th {{Nordic Conference}} on {{Computational Linguistics}} ({{NoDaLiDa}})},
  author = {Samuel, David and Kutuzov, Andrey and Touileb, Samia and Velldal, Erik and {\O}vrelid, Lilja and R{\o}nningstad, Egil and Sigdel, Elina and Palatkina, Anna},
  year = {2023},
  month = may,
  pages = {618--633},
  publisher = {{University of Tartu Library}},
  address = {{T\'orshavn, Faroe Islands}},
  urldate = {2023-10-16},
  abstract = {We present NorBench: a streamlined suite of NLP tasks and probes for evaluating Norwegian language models (LMs) on standardized data splits and evaluation metrics. We also introduce a range of new Norwegian language models (both encoder and encoder-decoder based). Finally, we compare and analyze their performance, along with other existing LMs, across the different benchmark tests of NorBench.},
  file = {C:\Users\oskar\Zotero\storage\XZJREB9E\Samuel et al. - 2023 - NorBench – A Benchmark for Norwegian Language Mode.pdf}
}

@inproceedings{scherrerHeLjuVarDial20202020,
  title = {{{HeLju}}@{{VarDial}} 2020: {{Social Media Variety Geolocation}} with {{BERT Models}}},
  shorttitle = {{{HeLju}}@{{VarDial}} 2020},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Scherrer, Yves and Ljube{\v s}i{\'c}, Nikola},
  year = {2020},
  month = dec,
  pages = {202--211},
  publisher = {{International Committee on Computational Linguistics (ICCL)}},
  address = {{Barcelona, Spain (Online)}},
  urldate = {2023-10-16},
  abstract = {This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our models reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional models by far, and that improvements obtained by pre-training models on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction: named entities and dialectal features, both of which are handled well by our models.},
  file = {C:\Users\oskar\Zotero\storage\SXY6K8F7\Scherrer and Ljubešić - 2020 - HeLju@VarDial 2020 Social Media Variety Geolocati.pdf}
}

@inproceedings{scherrerSocialMediaVariety2021,
  title = {Social {{Media Variety Geolocation}} with {{geoBERT}}},
  booktitle = {Proceedings of the {{Eighth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Scherrer, Yves and Ljube{\v s}i{\'c}, Nikola},
  year = {2021},
  month = apr,
  pages = {135--140},
  publisher = {{Association for Computational Linguistics}},
  address = {{Kiyv, Ukraine}},
  urldate = {2023-10-06},
  abstract = {This paper describes the Helsinki\textendash Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available.},
  file = {C:\Users\oskar\Zotero\storage\5RVDU6NE\Scherrer and Ljubešić - 2021 - Social Media Variety Geolocation with geoBERT.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-10},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\oskar\Zotero\storage\7YZDRE5Z\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}
