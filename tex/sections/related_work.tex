\section{Related Work}
\label{sec:related_work}

\begin{comment}
What other research has been conducted in this area and how is it related to your work?
This section is thus where your literature review will be presented. It is important when presenting the review
that you give an overview of the motivating elements of the work going on in your field and how these relate to your work,
rather than a list of contributors and what they have done.
This means that you need to extract the key important factors for your work and discuss how others have addressed
each of these factors and what the advantages/disadvantages are with such approaches
(objectively speaking or in the words of the authors themselves --- save your own views until the Discussion section).
As you mention other authors, you should reference their work.
Note that the reference list reflects the literature you have read \textit{and\/} have cited.
This will only be a subset of the literature that you have read.

A good way to find relevant work is by checking what others are referencing, e.g., in papers you have already found
or in previous studies carried out at NTNU, such as \citep{Berg;Gopinathan:17}.
However, when doing that,
do not fall into one of the common traps, such as re-iterating someone's false quote or faulty analysis of
a previous paper (check the original source!), or to get stuck inside a local research cluster (a group of
researchers that mainly refer to the ones using the same type of approaches or similar ideas).

Note that a reference needs to be complete: you should always give the full name of a conference or journal,
always include page numbers, always say where a book or thesis was published and where a conference took place.
Gather the full set of references together under the heading \textbf{References};
place the section before any Appendices, unless they contain references.
Arrange the references alphabetically by first author, rather than by order of occurrence in the text.
\end{comment}

\glsresetall

This work builds on top of the work of \cite{scherrerHeLjuVarDial20202020} and \cite{scherrerSocialMediaVariety2021}. They were the only participants in VarDial who used a large \acrshortpl{acr:lm} like \acrshort{acr:bert} in the shared task on social media variety geolocation, and did so with great success, winning the shared task in both \citeyear{scherrerHeLjuVarDial20202020} and \citeyear{scherrerSocialMediaVariety2021}. \citeauthor{scherrerHeLjuVarDial20202020} converted the task into a double regression problem, where latitude and longitude values are predicted from the output of a large \acrshort{acr:lm}. They experimented with different pre-trained models, coordinate encodings, and hyperparameters. Their main finding was that single-language models outperform multilingual models, the latter of which perform worse due to capacity dilution and tokeniers yielding suboptimal text splitting \citep[3]{scherrerHeLjuVarDial20202020}. As they were unable to find pre-trained model a pre-trained model for Swiss German they instead trained \texttt{bert-base-german-uncased}\footnote{\url{https://huggingface.co/dbmdz/bert-base-german-uncased}} (German \acrshort{acr:bert}) on the SwissCrawl corpus \citep{linderAutomaticCreationText2020}. Training a total of 48 models, \citeauthor{scherrerHeLjuVarDial20202020} were able to achieve a median distance of 15.72 km in this unconstrained setting using the default data split. They got a median distance of 15.45 km by using a substantioal portion of the development set for training \citep[6]{scherrerHeLjuVarDial20202020}.

\cite{gamanReportVarDialEvaluation2020} and \cite{chakravarthiFindingsVarDialEvaluation2021} summarize the findings in the \citeyear{gamanReportVarDialEvaluation2020} and \citeyear{chakravarthiFindingsVarDialEvaluation2021} editions of VarDial, including attempts made on the \gls{acr:smg} task. While \cite{scherrerHeLjuVarDial20202020} generally dominated the leaderboards, \cite{benitesdeazevedoesouzaZHAWInITSocialMedia2020} proposed a method that performed best among constrained submissions on the Swiss task \citep[8-9]{gamanReportVarDialEvaluation2020}, and only marginally worse than \citeauthor{scherrerHeLjuVarDial20202020}'s unconstrained submissions. \cite{benitesdeazevedoesouzaZHAWInITSocialMedia2020} use K-Means clustering \citep{lloydLeastSquaresQuantization1982a} of locations and predicting cluster identities, framing the problem as a classification task rather than a regression task. Their best submission extracts features from different levels of token granularity, training a separate \acrshort{acr:svm} for each feature set, before feeding the distances to the decision boundaries for each feature classifier as input to a \acrshort{acr:svm} meta-classifier.