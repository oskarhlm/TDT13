{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT13 Project - Oskar Holm (F2023)\n",
    "\n",
    "This project is based on the shared task related to Social Media Geolocation (SMG) from VarDial 2020 and 2021, specifically the Workshop on Natural Language Processing (NLP) for Similar Languages, Varieties, and Dialects. Unlike typical VarDial tasks that involve choosing from a set of variety labels, this task focuses on predicting the latitude and longitude from which a social media post was made.\n",
    "\n",
    "The task remained the same in both 2020 and 2021, covering three language areas: Bosnian-Croatian-Montenegrin-Serbian, German (Germany and Austria), and German-speaking Switzerland. This project is limited to the German-speaking Switzerland area due to time constraints and resource availability.\n",
    "\n",
    "The goal of the project is to replicate the results of a study that used a BERT-based classifier for this double regression task. The dataset from the 2020 VarDial challenge is chosen because it had more submissions compared to the 2021 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json \n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.scalers import scalers\n",
    "from lib.train_utils import TensorBoardCheckpoint\n",
    "from lib.geo_utils import mean_distance, median_distance, to_projection, GeolocationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_data': 'SMG2020/ch/train.txt',\n",
       " 'dev_data': 'SMG2020/ch/dev.txt',\n",
       " 'scaler': 'joint',\n",
       " 'model_type': 'bert',\n",
       " 'model_name': 'statworx/bert-base-german-cased-finetuned-swiss',\n",
       " 'lossfn': 'MAELoss',\n",
       " 'save_predictions': True,\n",
       " 'train_batch_size': 32,\n",
       " 'max_seq_length': 128,\n",
       " 'epochs': 50,\n",
       " 'lr': 2e-05,\n",
       " 'seed': 42,\n",
       " 'projection': 'utm',\n",
       " 'zone_number': 32,\n",
       " 'zone_letter': 'T'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_name = 'utm_lr2e-5'\n",
    "with open('./configs.json', 'r') as f: \n",
    "    config = json.load(f)[config_name]\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>easting</th>\n",
       "      <th>northing</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381131.804856</td>\n",
       "      <td>5.230808e+06</td>\n",
       "      <td>Dr Chester Bennington isch tot ðŸ˜”ðŸ˜”ðŸ˜” #rip #linki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439783.213452</td>\n",
       "      <td>5.189910e+06</td>\n",
       "      <td>Mini FrÃ¼ndin hed Lust uf Doktorspieli gha... ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>438114.281248</td>\n",
       "      <td>5.248831e+06</td>\n",
       "      <td>Slayer isch besser. Det han ich gescht mini Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>483390.191173</td>\n",
       "      <td>5.246306e+06</td>\n",
       "      <td>gaht au innere stund? bin grad am speck brate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>427548.501983</td>\n",
       "      <td>5.248952e+06</td>\n",
       "      <td>sie: thy er: ? sie: thy= thank you er: player ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22595</th>\n",
       "      <td>487904.163647</td>\n",
       "      <td>5.238515e+06</td>\n",
       "      <td>Bin grad in Bus igstige, da seit de Buschauffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22596</th>\n",
       "      <td>365623.151806</td>\n",
       "      <td>5.181110e+06</td>\n",
       "      <td>Rien ne surpassera Dragostea Din Tei de O-zone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22597</th>\n",
       "      <td>543374.940570</td>\n",
       "      <td>5.199765e+06</td>\n",
       "      <td>het Ã¶pert au kei bock meh zum schaffa und lust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22598</th>\n",
       "      <td>419850.326150</td>\n",
       "      <td>5.237936e+06</td>\n",
       "      <td>Oh wenn wedermol en jodel -5 het wos ned verdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22599</th>\n",
       "      <td>446619.035471</td>\n",
       "      <td>5.270976e+06</td>\n",
       "      <td>Zerst hani glachet das min Kollege Ã¤ndlich au ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22600 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             easting      northing  \\\n",
       "0      381131.804856  5.230808e+06   \n",
       "1      439783.213452  5.189910e+06   \n",
       "2      438114.281248  5.248831e+06   \n",
       "3      483390.191173  5.246306e+06   \n",
       "4      427548.501983  5.248952e+06   \n",
       "...              ...           ...   \n",
       "22595  487904.163647  5.238515e+06   \n",
       "22596  365623.151806  5.181110e+06   \n",
       "22597  543374.940570  5.199765e+06   \n",
       "22598  419850.326150  5.237936e+06   \n",
       "22599  446619.035471  5.270976e+06   \n",
       "\n",
       "                                                    text  \n",
       "0      Dr Chester Bennington isch tot ðŸ˜”ðŸ˜”ðŸ˜” #rip #linki...  \n",
       "1      Mini FrÃ¼ndin hed Lust uf Doktorspieli gha... ....  \n",
       "2      Slayer isch besser. Det han ich gescht mini Dr...  \n",
       "3      gaht au innere stund? bin grad am speck brate ...  \n",
       "4      sie: thy er: ? sie: thy= thank you er: player ...  \n",
       "...                                                  ...  \n",
       "22595  Bin grad in Bus igstige, da seit de Buschauffe...  \n",
       "22596  Rien ne surpassera Dragostea Din Tei de O-zone...  \n",
       "22597  het Ã¶pert au kei bock meh zum schaffa und lust...  \n",
       "22598  Oh wenn wedermol en jodel -5 het wos ned verdi...  \n",
       "22599  Zerst hani glachet das min Kollege Ã¤ndlich au ...  \n",
       "\n",
       "[22600 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_table('vardial-shared-tasks/SMG2020/ch/train.txt', header=None, names=['lat', 'lon', 'text'])\n",
    "dev_data = pd.read_table('vardial-shared-tasks/SMG2020/ch/dev.txt', header=None, names=['lat', 'lon', 'text'])\n",
    "\n",
    "train_data, col_names = to_projection(train_data, config)\n",
    "dev_data, _ = to_projection(dev_data, config)\n",
    "\n",
    "scaler = scalers[config['scaler']]()\n",
    "\n",
    "train_coords = scaler.fit_transform(train_data[col_names[:2]].values)\n",
    "train_dataset = GeolocationDataset(train_data['text'].tolist(), train_coords, config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True)\n",
    "\n",
    "Path('data/ch').mkdir(exist_ok=True)\n",
    "with open('data/ch/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "dev_coords = scaler.transform(dev_data[col_names[:2]].values)\n",
    "dev_dataset = GeolocationDataset(dev_data['text'].tolist(), dev_coords, config)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), config['lr'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = L1Loss()\n",
    "\n",
    "tb_checkpoint = TensorBoardCheckpoint(log_dir='data/ch/logs',\n",
    "                                      checkpoint_path='data/ch/checkpoints', run_name=config_name)\n",
    "\n",
    "def train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=10):\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on dev (validation)\n",
    "        model.eval()\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader):\n",
    "                inputs, labels = batch\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(**inputs)\n",
    "                dev_preds.append(outputs.logits.cpu().numpy())\n",
    "                dev_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        dev_preds = np.vstack(dev_preds)\n",
    "        dev_labels = np.vstack(dev_labels)\n",
    "\n",
    "        median_dist = median_distance(dev_preds, dev_labels, scaler)\n",
    "        mean_dist = mean_distance(dev_preds, dev_labels, scaler)\n",
    "\n",
    "        metrics = {'Loss/train': avg_train_loss, 'Median_Distance/dev': median_dist, 'Mean_Distance/dev': mean_dist}\n",
    "        tb_checkpoint.log_metrics(metrics, epoch)\n",
    "        tb_checkpoint.save_checkpoint(model, optimizer, epoch, metrics, scaler)\n",
    "\n",
    "with open('data/ch/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=config['epochs'])\n",
    "\n",
    "tb_checkpoint.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_geolocation_model(checkpoint_dir, checkpoint_file, train_data, test_gold_data, config, model, device):\n",
    "    checkpoint_path = f'{checkpoint_dir}/{checkpoint_file}_best_model.pth'\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(checkpoint['metrics'])\n",
    "\n",
    "    checkpoint_scaler = checkpoint['scaler']\n",
    "    checkpoint_scaler.fit(train_data[['easting', 'northing']].values)\n",
    "\n",
    "    test_gold_coords = checkpoint_scaler.transform(test_gold_data[['easting', 'northing']].values)\n",
    "    test_gold_dataset = GeolocationDataset(test_gold_data['text'].tolist(), test_gold_coords)\n",
    "    test_gold_loader = DataLoader(test_gold_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for batch in tqdm(test_gold_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            test_preds.append(logits.cpu().numpy())\n",
    "\n",
    "    test_preds = np.concatenate(test_preds, axis=0)\n",
    "\n",
    "    results = {\n",
    "        'median_distance': median_distance(test_gold_coords, test_preds, checkpoint_scaler, config),\n",
    "        'mean_distance': mean_distance(test_gold_coords, test_preds, checkpoint_scaler, config)\n",
    "    }\n",
    "\n",
    "    print(f'{checkpoint_file}: {results}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'data/ch/checkpoints'\n",
    "\n",
    "# for checkpoint_file in os.listdir(checkpoint_dir):\n",
    "#     checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#     checkpoint_scaler = checkpoint['scaler']()\n",
    "#     checkpoint_scaler.fit(train_data[['lat', 'lon']].values)\n",
    "\n",
    "#     test_gold_coords = checkpoint_scaler.transform(test_gold_data[['lat', 'lon']].values)\n",
    "#     test_gold_dataset = GeolocationDataset(test_gold_data['text'].tolist(), test_gold_coords)\n",
    "#     test_gold_loader = DataLoader(test_gold_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_preds = []\n",
    "#         for batch in tqdm(test_gold_loader):\n",
    "#             inputs, labels = batch\n",
    "#             inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             test_preds.append(logits.cpu().numpy())\n",
    "\n",
    "#     test_preds = np.concatenate(test_preds, axis=0)\n",
    "\n",
    "#     results = {\n",
    "#         'median_distance': median_distance(test_gold_coords, test_preds, checkpoint_scaler),\n",
    "#         'mean_distance': mean_distance(test_gold_coords, test_preds, checkpoint_scaler)\n",
    "#     }\n",
    "\n",
    "#     print(f'{checkpoint_file}: {results}\\n')\n",
    "\n",
    "#     if results['median_distance'] < best_results['median_distance']:\n",
    "#         best_checkpoint = checkpoint_file\n",
    "#         best_results['median_distance'] = results['median_distance']\n",
    "#         best_results['mean_distance'] = results['mean_distance']\n",
    "\n",
    "# print(\"\\nBest Checkpoint:\", best_checkpoint)\n",
    "# print(\"Best Results:\", best_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
