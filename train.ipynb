{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"train_data\": \"SMG2020/ch/train.txt\",\n",
    "    \"dev_data\": \"SMG2020/ch/dev.txt\",\n",
    "    \"ynorm\": \"jointscale\",\n",
    "    \"model_type\": \"bert\",\n",
    "    \"model_name\": \"statworx/bert-base-german-cased-finetuned-swiss\",\n",
    "    \"lossfn\": \"MAELoss\",\n",
    "    \"save_predictions\": True,\n",
    "    \"num_train_epochs\": 50,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 2e-5,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeolocationDataset(Dataset):\n",
    "    def __init__(self, texts, coordinates):\n",
    "        self.texts = texts\n",
    "        self.coordinates = coordinates\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, return_tensors='pt')\n",
    "        inputs = {key: val.squeeze() for key, val in inputs.items()}  # Remove batch dimension\n",
    "        coords = torch.tensor(self.coordinates[idx], dtype=torch.float)\n",
    "        return inputs, coords\n",
    "\n",
    "\n",
    "col_names = ['lat', 'lon', 'text']\n",
    "train_data = pd.read_csv('vardial-shared-tasks/SMG2020/ch/train.txt',\n",
    "                         delimiter='\\t', header=None, names=col_names)\n",
    "dev_data = pd.read_csv('vardial-shared-tasks/SMG2020/ch/dev.txt',\n",
    "                       delimiter='\\t', header=None, names=col_names)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_coords = scaler.fit_transform(train_data[['lat', 'lon']].values)\n",
    "dev_coords = scaler.transform(dev_data[['lat', 'lon']].values)\n",
    "\n",
    "# Save scaler for later use\n",
    "Path('data/ch').mkdir(exist_ok=True)\n",
    "with open('data/ch/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "train_dataset = GeolocationDataset(train_data['text'].tolist(), train_coords)\n",
    "dev_dataset = GeolocationDataset(dev_data['text'].tolist(), dev_coords)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config['train_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6371  # Radius of Earth in kilometers\n",
    "\n",
    "def haversine_distance(c1, c2):\n",
    "    # c1 and c2 are arrays containing latitude and longitude in degrees\n",
    "    c1 = np.radians(c1)\n",
    "    c2 = np.radians(c2)\n",
    "    dlat = c2[:, 0] - c1[:, 0]\n",
    "    dlon = c2[:, 1] - c1[:, 1]\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(c1[:, 0]) * np.cos(c2[:, 0]) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return R * c  # Distance in kilometers\n",
    "\n",
    "def median_distance(preds, labels, scaler=scaler):\n",
    "    if scaler:\n",
    "        preds = scaler.inverse_transform(preds)\n",
    "        labels = scaler.inverse_transform(labels)\n",
    "    return np.median(haversine_distance(preds, labels))\n",
    "\n",
    "def mean_distance(preds, labels, scaler):\n",
    "    if scaler:\n",
    "        preds = scaler.inverse_transform(preds)\n",
    "        labels = scaler.inverse_transform(labels)\n",
    "    return np.mean(haversine_distance(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at statworx/bert-base-german-cased-finetuned-swiss and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\koholm\\dev\\TDT13\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), config['lr'])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class TensorBoardCheckpoint:\n",
    "    def __init__(self, log_dir, checkpoint_path, best_only=True):\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_metric = float('inf')\n",
    "        self.best_only = best_only\n",
    "\n",
    "    def log_metrics(self, metrics, step):\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self.writer.add_scalar(metric_name, metric_value, step)\n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, epoch, metrics):\n",
    "        checkpoint_path = os.path.join(self.checkpoint_path, 'best_model_checkpoint.pth')\n",
    "        if metrics['Median_Distance/dev'] < self.best_metric:\n",
    "            self.best_metric = metrics['Median_Distance/dev']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "            }, checkpoint_path)\n",
    "            self.writer.add_text('Best_Checkpoint', self.checkpoint_path, epoch)\n",
    "            print(f\"New best checkpoint saved at {self.checkpoint_path}\")\n",
    "        elif not self.best_only:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/707 [00:30<1:59:58, 10.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\koholm\\dev\\TDT13\\train.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata/ch/scaler.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     scaler \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m tb_checkpoint\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32mc:\\Users\\koholm\\dev\\TDT13\\train.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs\u001b[39m.\u001b[39mlogits, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/koholm/dev/TDT13/train.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "loss_function = L1Loss()\n",
    "\n",
    "run_dir = f'data/ch/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "tb_checkpoint = TensorBoardCheckpoint(log_dir=f'{run_dir}/tensorboard_logs',\n",
    "                                      checkpoint_path=f'{run_dir}')\n",
    "\n",
    "def compute_median_distance(preds, labels, scaler):\n",
    "    preds = scaler.inverse_transform(preds)\n",
    "    labels = scaler.inverse_transform(labels)\n",
    "    return median_distance(preds, labels)\n",
    "\n",
    "def train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=10):\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on the development set\n",
    "        model.eval()\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader):\n",
    "                inputs, labels = batch\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(**inputs)\n",
    "                dev_preds.append(outputs.logits.cpu().numpy())\n",
    "                dev_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        # Concatenate all the predictions and labels\n",
    "        dev_preds = np.vstack(dev_preds)\n",
    "        dev_labels = np.vstack(dev_labels)\n",
    "\n",
    "        median_dist = median_distance(dev_preds, dev_labels, scaler)\n",
    "        mean_dist = mean_distance(dev_preds, dev_labels, scaler)\n",
    "\n",
    "        metrics = {'Loss/train': avg_train_loss, 'Median_Distance/dev': median_dist, 'Mean_Distance/dev': mean_dist}\n",
    "        tb_checkpoint.log_metrics(metrics, epoch)\n",
    "        tb_checkpoint.save_checkpoint(model, optimizer, epoch, metrics)\n",
    "\n",
    "with open('data/ch/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=config['epochs'])\n",
    "\n",
    "tb_checkpoint.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
