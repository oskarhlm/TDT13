{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"train_data\": \"SMG2020/ch/train.txt\",\n",
    "    \"dev_data\": \"SMG2020/ch/dev.txt\",\n",
    "    \"scaler\": \"joint\",\n",
    "    \"model_type\": \"bert\",\n",
    "    \"model_name\": \"statworx/bert-base-german-cased-finetuned-swiss\",\n",
    "    \"lossfn\": \"MAELoss\",\n",
    "    \"save_predictions\": True,\n",
    "    \"num_train_epochs\": 50,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 2e-5,\n",
    "    \"seed\": 42,\n",
    "    'projection': 'utm'\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointScaler():\n",
    "\tdef __init__(self):\n",
    "\t\tself.means = None\n",
    "\t\tself.stddev = None\n",
    "\n",
    "\tdef fit_transform(self, data):\n",
    "\t\tself.fit(data)\n",
    "\t\treturn self.transform(data)\n",
    "\t\n",
    "\tdef fit(self, data):\n",
    "\t\tself.means = np.mean(data, axis=0)\n",
    "\t\tcentereddata = data - self.means\n",
    "\t\tself.stddev = np.std(centereddata)\n",
    "\n",
    "\tdef transform(self, data):\n",
    "\t\treturn (data - self.means) / self.stddev\n",
    "\n",
    "\tdef inverse_transform(self, data):\n",
    "\t\treturn (data * self.stddev) + self.means\n",
    "\t\n",
    "\n",
    "scalers = {\n",
    "\t'independent': StandardScaler,\n",
    "\t'joint': JointScaler\n",
    "}\n",
    "\n",
    "scaler = scalers[config['scaler']]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeolocationDataset(Dataset):\n",
    "    def __init__(self, texts, coordinates):\n",
    "        self.texts = texts\n",
    "        self.coordinates = coordinates\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, return_tensors='pt')\n",
    "        inputs = {key: val.squeeze() for key, val in inputs.items()}  # Remove batch dimension\n",
    "        coords = torch.tensor(self.coordinates[idx], dtype=torch.float)\n",
    "        return inputs, coords\n",
    "\n",
    "\n",
    "col_names = ['lat', 'lon', 'text']\n",
    "\n",
    "train_data = pd.read_csv('vardial-shared-tasks/SMG2020/ch/train.txt',\n",
    "                         delimiter='\\t', header=None, names=col_names)\n",
    "dev_data = pd.read_csv('vardial-shared-tasks/SMG2020/ch/dev.txt',\n",
    "                       delimiter='\\t', header=None, names=col_names)\n",
    "\n",
    "\n",
    "\n",
    "train_coords = scaler.fit_transform(train_data[['lat', 'lon']].values)\n",
    "train_dataset = GeolocationDataset(train_data['text'].tolist(), train_coords)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True)\n",
    "\n",
    "Path('data/ch').mkdir(exist_ok=True)\n",
    "with open('data/ch/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "dev_coords = scaler.transform(dev_data[['lat', 'lon']].values)\n",
    "dev_dataset = GeolocationDataset(dev_data['text'].tolist(), dev_coords)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config['train_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         (381131.8048563974, 5230807.862731854, 32, T)\n",
       "1         (439783.2134515929, 5189909.538309485, 32, T)\n",
       "2         (438114.2812478363, 5248830.813624965, 32, T)\n",
       "3         (483390.1911730778, 5246305.664604473, 32, T)\n",
       "4         (427548.5019832254, 5248951.614359201, 32, T)\n",
       "                              ...                      \n",
       "22595     (487904.1636466054, 5238515.311577261, 32, T)\n",
       "22596      (365623.15180610865, 5181109.5548293, 32, T)\n",
       "22597     (543374.9405695203, 5199765.483507183, 32, T)\n",
       "22598    (419850.32615028275, 5237936.389318025, 32, T)\n",
       "22599     (446619.0354707878, 5270976.269557163, 32, T)\n",
       "Length: 22600, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utm\n",
    "\n",
    "zone_number = 32\n",
    "zone_letter = 'T'\n",
    "\n",
    "train_data.apply(lambda row: utm.from_latlon(\n",
    "    row['lat'], row['lon'], force_zone_number=zone_number, force_zone_letter=zone_letter), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6371  # Radius of Earth [km]\n",
    "\n",
    "def haversine_distance(c1, c2):\n",
    "    # c1 and c2 are arrays containing lat/lon in degrees\n",
    "    c1 = np.radians(c1)\n",
    "    c2 = np.radians(c2)\n",
    "    dlat = c2[:, 0] - c1[:, 0]\n",
    "    dlon = c2[:, 1] - c1[:, 1]\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(c1[:, 0]) * np.cos(c2[:, 0]) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return R * c  # Distance [km]\n",
    "\n",
    "def median_distance(preds, labels, scaler=scaler):\n",
    "    if scaler:\n",
    "        preds = scaler.inverse_transform(preds)\n",
    "        labels = scaler.inverse_transform(labels)\n",
    "    return np.median(haversine_distance(preds, labels))\n",
    "\n",
    "def mean_distance(preds, labels, scaler=scaler):\n",
    "    if scaler:\n",
    "        preds = scaler.inverse_transform(preds)\n",
    "        labels = scaler.inverse_transform(labels)\n",
    "    return np.mean(haversine_distance(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at statworx/bert-base-german-cased-finetuned-swiss and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\koholm\\dev\\TDT13\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), config['lr'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class TensorBoardCheckpoint:\n",
    "    def __init__(self, log_dir, checkpoint_path, best_only=True):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_metric = float('inf')\n",
    "        self.best_only = best_only\n",
    "        self.date_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_dir = f'{log_dir}/{self.date_str}'\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "        self.metric_path = f'{self.log_dir}/metrics.csv'\n",
    "\n",
    "    def log_metrics(self, metrics, step):\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self.writer.add_scalar(metric_name, metric_value, step)\n",
    "\n",
    "        with open(self.metric_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if not os.path.isfile(self.metric_path):\n",
    "                writer.writerow(metrics.keys())\n",
    "\n",
    "            writer.writerow(metrics.values()) \n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, epoch, metrics, scaler):\n",
    "        checkpoint_path = f'{self.checkpoint_path}/{self.date_str}_best_model.pth'\n",
    "        if metrics['Median_Distance/dev'] < self.best_metric:\n",
    "            self.best_metric = metrics['Median_Distance/dev']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'scaler': scaler\n",
    "            }, checkpoint_path)\n",
    "            self.writer.add_text('New_Best_Checkpoint', self.checkpoint_path, epoch)\n",
    "            print(f\"New best checkpoint saved at {self.checkpoint_path}\")\n",
    "        elif not self.best_only:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "loss_function = L1Loss()\n",
    "\n",
    "tb_checkpoint = TensorBoardCheckpoint(log_dir='data/ch/logs',\n",
    "                                      checkpoint_path='data/ch/checkpoints')\n",
    "\n",
    "def compute_median_distance(preds, labels, scaler):\n",
    "    preds = scaler.inverse_transform(preds)\n",
    "    labels = scaler.inverse_transform(labels)\n",
    "    return median_distance(preds, labels)\n",
    "\n",
    "def train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=10):\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = loss_function(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on dev (validation)\n",
    "        model.eval()\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_loader):\n",
    "                inputs, labels = batch\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(**inputs)\n",
    "                dev_preds.append(outputs.logits.cpu().numpy())\n",
    "                dev_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        dev_preds = np.vstack(dev_preds)\n",
    "        dev_labels = np.vstack(dev_labels)\n",
    "\n",
    "        median_dist = median_distance(dev_preds, dev_labels, scaler)\n",
    "        mean_dist = mean_distance(dev_preds, dev_labels, scaler)\n",
    "\n",
    "        metrics = {'Loss/train': avg_train_loss, 'Median_Distance/dev': median_dist, 'Mean_Distance/dev': mean_dist}\n",
    "        tb_checkpoint.log_metrics(metrics, epoch)\n",
    "        tb_checkpoint.save_checkpoint(model, optimizer, epoch, metrics, scaler)\n",
    "\n",
    "with open('data/ch/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "train(model, train_loader, dev_loader, optimizer, loss_function, scaler, epochs=config['epochs'])\n",
    "\n",
    "tb_checkpoint.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:14<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231107-230346_best_model.pth: {'median_distance': 16.44642298143306, 'mean_distance': 23.11484565124471}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:14<00:00,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231108-155200_best_model.pth: {'median_distance': 16.570519292930843, 'mean_distance': 23.857210359269036}\n",
      "Best Checkpoint: data/ch/checkpoints\\20231107-230346_best_model.pth\n",
      "Best Results: {'median_distance': 16.44642298143306, 'mean_distance': 23.11484565124471}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "checkpoint_dir = 'data/ch/checkpoints'\n",
    "\n",
    "best_checkpoint = None\n",
    "best_results = {\n",
    "    'median_distance': float('inf'),\n",
    "    'mean_distance': float('inf')\n",
    "}\n",
    "\n",
    "test_gold_data = pd.read_csv('vardial-shared-tasks/SMG2020/ch/test_gold.txt',\n",
    "                       delimiter='\\t', header=None, names=col_names)\n",
    "\n",
    "for checkpoint_file in os.listdir(checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    checkpoint_scaler = checkpoint['scaler']()\n",
    "    checkpoint_scaler.fit(train_data[['lat', 'lon']].values)\n",
    "\n",
    "    test_gold_coords = checkpoint_scaler.transform(test_gold_data[['lat', 'lon']].values)\n",
    "    test_gold_dataset = GeolocationDataset(test_gold_data['text'].tolist(), test_gold_coords)\n",
    "    test_gold_loader = DataLoader(test_gold_dataset, batch_size=config['train_batch_size'], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for batch in tqdm(test_gold_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            test_preds.append(logits.cpu().numpy())\n",
    "\n",
    "    test_preds = np.concatenate(test_preds, axis=0)\n",
    "\n",
    "    results = {\n",
    "        'median_distance': median_distance(test_gold_coords, test_preds, checkpoint_scaler),\n",
    "        'mean_distance': mean_distance(test_gold_coords, test_preds, checkpoint_scaler)\n",
    "    }\n",
    "\n",
    "    print(f'{checkpoint_file}: {results}\\n')\n",
    "\n",
    "    if results['median_distance'] < best_results['median_distance']:\n",
    "        best_checkpoint = checkpoint_file\n",
    "        best_results['median_distance'] = results['median_distance']\n",
    "        best_results['mean_distance'] = results['mean_distance']\n",
    "\n",
    "print(\"\\nBest Checkpoint:\", best_checkpoint)\n",
    "print(\"Best Results:\", best_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
